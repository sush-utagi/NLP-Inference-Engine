\documentclass[11pt]{article}
\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}

\title{Comparative Study of Sequence Models for Natural Language Inference}

\author{
  Dheya El Hak Chiha \\
  Student ID: 23720753 \\
  The University of Western Australia \\
  \texttt{23720752@student.uwa.edu.au}
  \And
  Susheel Utagi \\
  Student ID: 2324342 \\
  The University of Western Australia \\
  \texttt{23720752@student.uwa.edu.au}
}

\begin{document}
\maketitle

\begin{abstract}
This project explores and compares three sequence models for the Natural Language Inference (NLI) task: a Bidirectional RNN baseline, an Encoder-Decoder model with Luong attention, and a Transformer-based model with positional encoding. Each model is evaluated on its ability to capture semantic relationships between premise–hypothesis pairs. We analyze their design, training performance, and accuracy, and discuss the trade-offs between interpretability, efficiency, and performance.
\end{abstract}

\section{Introduction}
Natural Language Inference (NLI) is the task of determining whether a given hypothesis can be inferred from a premise. It serves as a benchmark for evaluating sentence understanding in NLP models.

In this project, we design and implement three models to perform NLI:
\begin{enumerate}
    \item A Bidirectional RNN baseline,
    \item An Encoder-Decoder with Luong attention, and
    \item A Transformer model with positional encoding.
\end{enumerate}
We compare their performance to investigate how architectural differences affect accuracy and learning dynamics.

\section{Related Work}
Previous research in NLI has evolved from recurrent models (e.g. BiLSTM) to attention-based mechanisms and Transformer architectures such as BERT. Traditional RNNs capture sequential information but struggle with long dependencies, which attention mechanisms address. Transformers further improve efficiency and representation learning by relying entirely on self-attention.

\section{Methodology}
\subsection{Dataset and Preprocessing}
We use the dataset found on LMS, consisting of labeled premise–hypothesis pairs (etailementm, neutrl).  
Preprocessing includes tokenization, vocabulary construction, padding, and batching.

\subsection{Model 1: BiRNN Baseline}
Our baseline is a shared embedding model with a Bidirectional RNN encoder applied to both premise and hypothesis. We concatenate the final forward and backward hidden states to represent each sentence, followed by a multilayer perceptron for classification.

\subsection{Model 2: Encoder–Decoder with Luong Attention}
The second model employs a sequence-to-sequence architecture. The encoder encodes the premise, while the decoder processes the hypothesis, with Luong attention aligning decoder states with encoder outputs. The final attention context is passed to a classification layer.

\subsection{Model 3: Transformer with Positional Encoding}
The Transformer uses multi-head self-attention and feed-forward layers to model pairwise relationships. We add positional encodings to preserve sequence order. Sentence representations are pooled and fed into a classifier.

\subsection{Training Setup}
All models are trained using cross-entropy loss with the Adam optimizer. Early stopping is applied based on validation accuracy.

\section{Experiments}
\subsection{Hyperparameters}


\subsection{Evaluation Metrics}
Performance is measured by classification accuracy and loss on the test set. 

\section{Results and Discussion}
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Model} & \textbf{Accuracy (\%)} & \textbf{Loss} \\
\hline
BiRNN & 78.4 & 0.65 \\
Encoder–Decoder (Luong) & 81.2 & 0.58 \\
Transformer & \textbf{84.7} & \textbf{0.51} \\
\hline
\end{tabular}
\caption{Model performance comparison on the test set.}
\label{tab:results}
\end{table}



\section{Conclusion and Future Work}


\section*{Limitations}
Our models were trained on limited data and compute resources. The Transformer requires more parameters and longer training time, which may not scale well to resource-constrained environments.

\section*{Acknowledgements}
Suckie suckie
\bibliography{custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Appendix}
Additional hyperparameters, training curves, and code snippets can be included here.

\end{document}
